{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"%%capture\n!pip install --upgrade trax","execution_count":1,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Introduction"},{"metadata":{},"cell_type":"markdown","source":"Prior to the introduction of [Wide Residual Networks](https://arxiv.org/pdf/1605.07146.pdf) (WRNs) by Sergey Zagoruyko and Nikos Komodakis, deep residual networks were shown to have a fractional increase in performance but at the cost of **doubling** the number of layers. This led to the problem of diminishing feature reuse and overall made the models slow to train. WRNs showed that having a wider residual network leads to better performance and increased the then SOTA results on CIFAR, SVHN and COCO. \n\nIn this notebook we run through a simple demonstration of training a WideResnet on the `cifar10` dataset using the [Trax](https://github.com/google/trax) framework. Trax is an end-to-end library for deep learning that focuses on **clear code and speed**. It is actively used and maintained in the *Google Brain team*."},{"metadata":{},"cell_type":"markdown","source":"# Issues with Traditional Residual Networks"},{"metadata":{},"cell_type":"markdown","source":"## Diminishing Feature Reuse"},{"metadata":{},"cell_type":"markdown","source":"A Residual block with a identity mapping, which allows us to train very deep networks is a weakness. As the gradient flows through the network there is nothing to force it to go through the residual block weights and thus it can avoid learning during training. This only a few blocks can run valuable representations or many blocks could share very little information with small contributions to the final goal. This problem was tried to be addressed using a special case of dropout applied to residual blocks in which an identity scalar weight is added to each residual block on which dropout is applied."},{"metadata":{},"cell_type":"markdown","source":"# Importing Libraries"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import trax\nfrom trax import layers as tl\nfrom trax.supervised import training\nfrom trax.models.resnet import WideResnet\n\ntrax.fastmath.set_backend('tensorflow-numpy')","execution_count":2,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Downloading Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%capture\ntrain_stream = trax.data.TFDS('cifar10', keys=('image', 'label'), train=True)()\neval_stream = trax.data.TFDS('cifar10', keys=('image', 'label'), train=False)()","execution_count":3,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Batch Generator"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data_pipeline = trax.data.Serial(\n    trax.data.Shuffle(),\n    trax.data.Batch(64),\n    trax.data.AddLossWeights(),\n)\n\ntrain_batches_stream = train_data_pipeline(train_stream)\n\neval_data_pipeline = trax.data.Serial(\n    trax.data.Batch(64),\n    trax.data.AddLossWeights(),\n)\n\neval_batches_stream = eval_data_pipeline(eval_stream)","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_task = training.TrainTask(\n    labeled_data=train_batches_stream,\n    loss_layer=tl.CrossEntropyLoss(),\n    optimizer=trax.optimizers.Adam(0.01),\n    n_steps_per_checkpoint=1000,\n)\n\neval_task = training.EvalTask(\n    labeled_data=eval_batches_stream,\n    metrics=[tl.CrossEntropyLoss(), tl.Accuracy()],\n    n_eval_batches=20,\n)","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = tl.Serial(\n    WideResnet(),\n    tl.LogSoftmax()\n)\n\ntraining_loop = training.Loop(model, \n                              train_task, \n                              eval_tasks=[eval_task], \n                              output_dir='./cnn_model')\n\ntraining_loop.run(5000)","execution_count":9,"outputs":[{"output_type":"stream","text":"\nStep   1000: Ran 200 train steps in 141.04 secs\nStep   1000: train CrossEntropyLoss |  1.49697816\nStep   1000: eval  CrossEntropyLoss |  1.31979529\nStep   1000: eval          Accuracy |  0.52265625\n\nStep   2000: Ran 1000 train steps in 572.22 secs\nStep   2000: train CrossEntropyLoss |  1.16957819\nStep   2000: eval  CrossEntropyLoss |  1.08194559\nStep   2000: eval          Accuracy |  0.60468750\n\nStep   3000: Ran 1000 train steps in 565.36 secs\nStep   3000: train CrossEntropyLoss |  1.00960529\nStep   3000: eval  CrossEntropyLoss |  0.94915995\nStep   3000: eval          Accuracy |  0.64921875\n\nStep   4000: Ran 1000 train steps in 569.97 secs\nStep   4000: train CrossEntropyLoss |  0.90725946\nStep   4000: eval  CrossEntropyLoss |  0.89641787\nStep   4000: eval          Accuracy |  0.68984375\n\nStep   5000: Ran 1000 train steps in 570.14 secs\nStep   5000: train CrossEntropyLoss |  0.83230388\nStep   5000: eval  CrossEntropyLoss |  0.87516099\nStep   5000: eval          Accuracy |  0.69531250\n","name":"stdout"}]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}
